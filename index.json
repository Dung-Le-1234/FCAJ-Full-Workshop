[
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/4-eventparticipated/4.4-event4/",
	"title": "AWS Well-Architected Security Pillar ",
	"tags": [],
	"description": "",
	"content": "Summary Report: AWS Well-Architected Security Pillar (AWS Cloud Mastery Series #3) Event Objective Explore more about IAM services ,IAM identity center, SSO, SSP Learned Cloudtrail, Guarduty , Security hub and automation EventBridge Infrastructure Protection, ​VPC segmentation, private vs public placement Learn data Protection and Incident Response Speakers Le Vu Xuan Anh:AWS Cloud Club Captain HCMUTE First Cloud AI Journey Tran Duc Anh:AWS Cloud Club Captain SGU First Cloud AI Journey Tran Doan Cong Ly:AWS Cloud Club Captain PTIT First Cloud AI Journey Danh Hoang Hieu Nghi:AWS Cloud Club Captain HUFLIT First Cloud AI Journey Nguyen Tuan Thinh Cloud: Engineer Trainee First Cloud AI Journey Nguyen Do Thanh Dat:Cloud Engineer Trainee First Cloud AI Journey Mendel Grabski(Long): Ex Head of security and devops cloud security solution architect Tinh Truong: AWS Community Builder platform engineer at TymerX Key Highlights Identity \u0026amp; Access Management Learned core IAM concepts: Users, Roles, Policies, and why to avoid long-term credentials. Explored IAM Identity Center (SSO) with permission Covered essential security practices: MFA, credential rotation, and IAM Access Analyzer. Detection -Learned how CloudTrail enables full auditing across accounts\nExplored threat detection services: GuardDuty and Security Hub. Understood logging at multiple layers: VPC Flow Logs, ALB logs, S3 access logs. Built alerting and automation workflows using EventBridge. Infrastructure Protection -​ KMS: key policies, grants, rotation\n​Encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB ​Secrets Manager \u0026amp; Parameter Store — patterns rotation ​Data classification \u0026amp; access guardrails Incident Response -​ IR lifecycle AWS\ncompromised IAM key ​S3 public exposure ​- EC2 malware detection ​- Snapshot, isolation, evidence collection Auto-response with Lambda/Step Functions Event Experience Attending the AWS Well-Architected Security Pillar I am not only learn new services but also could see people from others university, learn more services, also able to see and meet foreigner work in aws and sharing how they work The event help me to know more about security, have oppoturnity to know more about incident and solution also would like to join cloudclub to have more experiences.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/4-eventparticipated/4.3-event3/",
	"title": "​DevOps with AWS",
	"tags": [],
	"description": "",
	"content": "Summary Report: ​DevOps on AWS (AWS Cloud Mastery Series #2) Event Objectives Explore more about DevOps services and DevOps\u0026rsquo;s mindset Know more about infrastructe as code Learn more container Service on AWS Learn more about monitoring and observability Speakers Bao Huynh – AWS Community Builders Thinh Nguyen – AWS Community Builders Vi Tran – AWS Community Builders Long Huynh – AWS Community Builders Quy Pham – AWS Community Builders Nghiem le – AWS Community Builders Key Highlights AWS CodeCommit Secure, scalable Git-based repository service on AWS Supports GitFlow, Trunk-Based, and feature-branch workflows Ideal for collaboration, version control, and automation workflows AWS CodeBuild Fully managed build service for compiling, testing, and packaging code Supports custom buildspec configurations and parallel builds Enables automated testing pipelines for CI/CD workflows AWS CodeDeploy Automates application deployments across EC2, Lambda, and on-premises Supports Blue/Green, Rolling, and Canary deployment strategies Reduces downtime and deployment risks with traffic-shifting controls AWS CodePipeline End-to-end CI/CD orchestration with integration across AWS DevOps tools Automates build, test, and deploy stages with full version tracking Ideal for continuous delivery, multi-environment pipelines, and workflow automation AWS CloudFormation Declarative templates for provisioning and managing AWS resources Stack management with drift detection and rollback capabilities Ensures reproducible, consistent infrastructure automation AWS CDK (Cloud Development Kit) Infrastructure as Code using familiar programming languages High-level constructs and reusable patterns for rapid development Integrates seamlessly with CloudFormation for deployment Container Services on AWS Docker fundamentals for microservices and containerization Amazon ECR for image storage, scanning, and lifecycle policies Amazon ECS \u0026amp; EKS for scalable, orchestrated container deployments AWS App Runner for simplified container-based application hosting Monitoring \u0026amp; Observability Amazon CloudWatch: metrics, logs, alarms, and dashboards AWS X-Ray: distributed tracing and performance analysis Enables full-stack monitoring and operational visibility DevOps Best Practices \u0026amp; Case Studies Deployment strategies: feature flags, A/B testing, and canary releases Automated testing integration within CI/CD pipelines Incident management workflows and effective postmortems Case studies: Startup and enterprise DevOps transformations Event Experience Attending the ​DevOps on AWS I gained more knowledge about Devops and other services in AWS, I can know how they work and the benefits that could help me improve my project The event help me know more about devops and try new services in AWS.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: ““AI-Driven Development Life Cycle: Reimagining Software Engineering (AWS Cloud Mastery Series #1) Event Objectives Present a market update on the AI/ML adoption and current trends within the local region. Provide hands-on training for developing end-to-end ML models using Amazon SageMaker. Conduct a technical deep dive into Generative AI capabilities on Amazon Bedrock, including its Foundation Models, Agents, and Guardrails. Provide essential skills training in Prompt Engineering and the deployment of Retrieval-Augmented Generation (RAG) systems. Speakers AWS Experts Team Key Highlights Program Introduction \u0026amp; Networking Market Insight: Shared an update on the current state of Artificial Intelligence and Machine Learning adoption across the Vietnam market. AWS AI/ML Services: The SageMaker Platform Unified ML Workflow: Detailed the complete machine learning lifecycle managed on Amazon SageMaker, covering data preparation, labeling, model training, tuning, and integration with MLOps for automated deployment. Studio Walkthrough: A live demonstration provided a clear view of the SageMaker Studio interface and its professional data science features. Generative AI Deep Dive with Amazon Bedrock Model Selection: Offered criteria and comparisons for selecting the optimal Foundation Models, such as Claude, Llama, and Titan, based on specific use case requirements. Prompt Optimization: Covered advanced Prompt Engineering techniques, including Chain-of-Thought reasoning and Few-shot learning. RAG Implementation: Demonstrated the \u0026ldquo;Retrieval -\u0026gt; Augmentation -\u0026gt; Generation\u0026rdquo; architecture, focusing on how to integrate internal Knowledge Bases to significantly improve AI response accuracy. Advanced Controls: Introduced Bedrock Agents for managing complex, multi-step tasks and Guardrails for ensuring content safety and regulatory compliance. Live Build: Successfully built a functional GenAI Chatbot prototype using Amazon Bedrock during the session. Key Takeaways Platform Strategy SageMaker\u0026rsquo;s Role: The ideal, robust platform for managing traditional, iterative Machine Learning development cycles with governance. Bedrock\u0026rsquo;s Advantage: Provides a fast, simplified API-based route to deploying Generative AI applications without the overhead of managing complex underlying infrastructure. Advanced Implementation Beyond Simple Chat: RAG and Agents are critical technologies that enable GenAI to move past basic interactions and solve complex, real-world business challenges and workflows. Compliance and Safety: Guardrails are an obligatory component for any enterprise deployment, ensuring the AI operates within defined ethical, safety, and regulatory boundaries. Applying to Work Standardize MLOps: Apply the learned SageMaker standards to automate the model lifecycle, tracking, and governance in ongoing projects. Develop RAG PoC: Conduct experiments to integrate proprietary company documents into a Bedrock Knowledge Base to build a context-aware information retrieval assistant. Improve AI Quality: Implement Chain-of-Thought prompting techniques to enhance the logical consistency and output quality of existing AI applications. Strategic Selection: Utilize the learned criteria to make informed decisions when selecting Foundation Models, balancing performance requirements against cost considerations. Event Experience This workshop provided a highly valuable balance between fundamental Machine Learning principles and contemporary Generative AI practices.\nPractical Application The SageMaker Studio walkthrough offered a clear visualization of a professional ML workflow. The Bedrock Chatbot demonstration was a highlight, showcasing the rapid development possible for powerful GenAI applications. Market Relevance The analysis of the local AI market helped frame my technical work within the broader industry trends and identified future opportunities. The event help me understand more about AI-Driven Development Life Cycle:\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/3-blogstranslated/3.3-blog3/",
	"title": "AWS Big Data Blog",
	"tags": [],
	"description": "",
	"content": "Enhance Search with Vector Embeddings and Amazon OpenSearch Service This post describes how organizations can enhance their existing search capabilities with vector embeddings using Amazon OpenSearch Service. We discuss limitations of traditional keyword search, how vector search enables intelligent and contextual results, and the business impact realized by organizations such as Amazon Prime Video, Juicebox, and Amazon Music.\nThis is the first post in a series that walks through implementing modern search applications using vector search, generative AI, and agentic AI.\nBackground: Why Traditional Search Falls Short Search technology has existed for more than 30 years. Traditional keyword search requires users to reformulate queries repeatedly because the system cannot interpret:\nContext Intent Language nuance Users act as translators between their real information needs and rigid keyword systems.\nWith the rise of LLMs and foundation models, semantic search enables:\nVector-based matching Query rewriting Multi-pass agentic workflows More intelligent and context-aware results As expectations evolve, organizations are augmenting keyword search with semantic understanding to deliver more intuitive results.\nIntent-Based Search: Why It Matters Enhancing traditional systems with embeddings helps search engines interpret why a user is searching, not just what they type.\nThis combined approach improves the system’s ability to:\nUnderstand natural language Interpret intent Handle variations in query phrasing Deliver more contextually relevant results Limitations of Traditional Keyword Search Although keyword engines remain foundational, organizations face growing challenges:\nMissed Opportunities and Inefficient Discovery Example:\nAmazon Prime Video users searching for “live soccer” were shown irrelevant documentaries due to keyword matching without semantic context.\nInability to Capture Query Nuance Juicebox struggled with recruiting search using Boolean logic; it couldn’t capture deeper intent.\nLimited Personalization Amazon Music needed personalization features layered on search to capture user preferences.\nBusiness Impact Inefficient search increases user effort and reduces satisfaction, as seen in both Juicebox and Prime Video.\nWhy Modern Search Applications Are Important Organizations are at an inflection point. Analysts predict a rapid shift from keyword-based interactions to AI-powered search through 2026.\nReal-world examples (Prime Video, Juicebox) demonstrate:\nIncreased relevance Higher customer satisfaction Better ability to understand intent Leaders are enhancing—not replacing—existing systems by layering vector and semantic capabilities on top of proven keyword infrastructure.\nTransforming Business Value with Vector Search Vector search expands traditional capabilities to support:\nConversational search Natural language questions Diverse content types Multi-modal search Real Customer Outcomes Juicebox\nLatency reduced: 700 ms → 250 ms Relevance improved: +35% more relevant candidates Handles 800M profiles with high accuracy Amazon Music\nScales to 1.05 billion vectors Peak 7,100 vector QPS globally Supports catalog of 100M songs How Vector Embeddings Improve User Experiences Consider an ecommerce food delivery app. A user searches:\n“Quick, healthy dinner with tofu, no dairy”\nTraditional keyword search fails due to:\nMissing synonyms (“quick” vs. “30-minute meals”) Lack of semantic understanding (“healthy” ≠ tagged metadata) Inability to infer absence of ingredients Vector search interprets:\n“Quick” → meals \u0026lt;30 minutes “Healthy” → nutrient-dense “No dairy” → exclude milk, cheese, butter This enables significantly more relevant and satisfying search results.\nConclusion This post introduced the value of incorporating vector search into existing applications. We covered:\nLimitations of keyword-based search Improvements enabled by vector embeddings Real business benefits proven by Amazon customers Adding vector search is a strategic way to:\nBoost engagement Improve satisfaction Future-proof applications Enable generative AI–powered search experiences What’s Next? In the next post of this series, we cover:\nAutomatic Semantic Enrichment Generating embeddings with Amazon Bedrock Building vector indexes in OpenSearch Service Combining vector + keyword search Step-by-step guidance and code samples Additional Resources See also:\nAmazon OpenSearch Service as a Vector Database Migration and modernization patterns in the AWS Migration Hub Best-practice posts: Save big on OpenSearch: Unleashing Intel AVX-512 for binary vector performance Optimize multimodal search using TwelveLabs Embed API Amazon OpenSearch Service vector database capabilities revisited "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/3-blogstranslated/3.2-blog2/",
	"title": "AWS Database Blog",
	"tags": [],
	"description": "",
	"content": "How Smartsheet Enhances Recommendations Using Amazon Neptune and Knowledge Graphs Smartsheet is a leading SaaS-based collaborative work management platform trusted by enterprises worldwide to manage projects, automate workflows, and drive collaboration at scale. Smartsheet, a long time AWS customer, has continuously evolved to meet the growing needs of its global user base. Today, Smartsheet is taking its platform to the next level by implementing Knowledge Graphs built on Amazon Neptune with the goal of delivering intelligent, proactive, and contextual insights from collaboration data in Smartsheet. This innovation aims to enhance customer experience, boost productivity, and drive long-term retention and growth.\nIn this post, we describe the Smartsheet Knowledge Graph, built in partnership between Smartsheet and AWS. The Smartsheet Knowledge Graph is a unified data model connecting people, content, and work in Smartsheet, representing how users interact with assets, content, and their collaborators. Simply put, it is a professional network tailored for each user within Smartsheet. Capturing this collaboration between users and their work helps unlock personalization, insights, and recommendations for each user.\nExisting Solution As a leading collaborative work management platform, Smartsheet scales to support some of the largest enterprises in the world. The platform houses a vast amount of collaboration data created by users as they plan, track, and execute work. The challenge confronted by the Smartsheet engineering team is modeling this data in a way that represents the collaboration between users and their work to surface intelligent, contextual insights and expose them within the product experience, while also ensuring scalability, security, and performance across the user base.\nSmartsheet uses best engineering practices to implement features as micro-services. As a result, each micro-service uses its own data store in a technology that is suited to the service architecture. Having data split across many different data stores and technologies makes it hard to analyze and map relationships between objects at scale in real time. A unified graph data model solves this challenge by creating object nodes and edges that define the relationships between these objects. Doing so allows Smartsheet to then traverse this graph data model to understand who a user’s close collaborators are and which assets are relevant to the user, enabling Smartsheet to provide more individualized, contextual guidance and solutions to users. Solving this data challenge is an important step in improving wayfinding and personalization throughout the Smartsheet platform.\nBefore developing the graph data model, unearthing relationships between users and their work in Smartsheet meant performing joins across tables in Snowflake, migrating the data to Amazon DynamoDB via AWS Lambda functions, and deriving insights using ML models that needed constant retraining as the data and relationships changed.\nThe flexibility of the graph data model allows Smartsheet to model relationships and keep these up to date as users collaborate on the platform, mapping relationships up to 4 degrees away from the user with a single optimized openCypher query on Neptune. For example, Smartsheet can now provide suggested contacts relevant to each user for sharing sheets, dashboards, and workspaces, or suggest relevant content to users based on context or priority.\nSolution Built with AWS Smartsheet partnered with AWS to develop a comprehensive knowledge graph platform that brings customer usage data in Snowflake and graph technologies together. At the heart of the solution is a knowledge graph powered by Neptune, designed to model complex relationships across Smartsheet’s collaboration work management platform. The architecture brings together several AWS services:\nAmazon Neptune provides the graph database foundation, enabling relationship-based queries across interconnected data. Snowflake stores the customer’s usage data of their Smartsheet Plan Amazon Elastic Container Service (Amazon ECS) Tasks transform and prepare data for graph ingestion and facilitate web service that exposes graph insights and recommendations using Cypher queries via internal APIs to Smartsheet services Based web service that exposes graph insights and recommendations using Cypher queries via internal APIs to Smartsheet services Amazon Simple Storage Service (Amazon S3) stores graph data CSV files for bulk loading to Neptune AWS Step Functions orchestrate ECS tasks to transform and load data to Neptune With this architecture, Smartsheet delivers contextual insights and personalized recommendations, helping users to discover their work and connections faster.\nFor Smartsheet, designing a graph data model that ensured logical separation of data amongst its customers was of paramount importance – we reviewed the “Multi-tenancy guidance for ISVs running Amazon Neptune databases” guidance and chose a pool model for labeled property graphs, that is every tenant’s (Smartsheet customer) data is stored in a single graph but is logically separated to achieve data partitioning.\nSmartsheet partnered with AWS to optimize the data ingestion process from Snowflake into Neptune using bulk loader operations for creating and updating nodes and edges in the graph. Deletes from the graph were optimized using Gremlin queries with per query timeouts and optimized batch processing in place. Having the ability to run both Gremlin and openCypher on the same graph allowed us to use the best approach for each job. In the future, our engineers will improve this further by moving away from the bulk loader to a transactional streaming workflow. To avoid graph sprawl and storing relationships that were not relevant, the team regularly pruned the graph nodes and edges that were inactive for extended periods of time retaining only the most recent and relevant activity.\nAt the enterprise scale of the Smartsheet platform, the graph data model grows quickly, reaching million scale orders of magnitude — over 70 million nodes and 150 million edges. At this scale, it becomes important to optimize graph queries so that Smartsheet can deliver insights within its SLA. The AWS solution architects partnered with the Smartsheet team to optimize the openCypher queries, using best practices such as using WITH statements, breaking down larger queries into subqueries, and optimizing date calculations over date fields previously stored as strings.\nPartnership Impact TThe collaboration between Smartsheet and AWS has delivered significant value for both companies. Through AWS’s 3-day Experience Based Acceleration (EBA) workshop, what began as a proof-of-concept, transformed into an accelerated initiative after leadership witnessed the transformative potential of graph-powered insights during the intensive collaborative engagement.\n“Our partnership with AWS and Amazon Neptune has given us a powerful foundation to innovate while delivering the scale, security, and reliability our customers expect. The Knowledge Graph we’ve built together is more than just a data model — it’s how we’re able to provide smarter, more personalized insights that help teams move from simply managing work to solving challenges and executing strategy with speed and precision.”\n– JB Brown, Vice President of Engineering at Smartsheet\n“By building its Knowledge Graph on Amazon Neptune, Smartsheet is unlocking the power of connected data to deliver more contextual, personalized experiences for customers. Together, we’re combining an innovative, flexible data model with the enterprise-grade scale and security of AWS, enabling organizations to work smarter and faster.”\n– Brad Bebee, Director at AWS\nSmartsheet recently released the first Knowledge Graph-powered feature (following image) to its Enterprise customers. While the following results are still early, in just a few weeks:\n1 in 7 Enterprise customers are using Knowledge Graph-powered suggestions to share content 99.9% of users share from the first 3 suggestions These suggestions help Smartsheet customers and users share faster and more easily, with the people they commonly collaborate with. With just a few clicks, this new yet simple feature helps reinforce and remind Smartsheet solution builders and admins who to share with.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/3-blogstranslated/3.1-blog1/",
	"title": "AWS Security Blog",
	"tags": [],
	"description": "",
	"content": "Securing generative AI: An introduction to the Generative AI Security Scoping Matrix Generative artificial intelligence (generative AI) has captured the imagination of organizations and is transforming the customer experience in industries of every size across the globe. This leap in AI capability, fueled by multi-billion-parameter large language models (LLMs) and transformer neural networks, has opened the door to new productivity improvements, creative capabilities, and more.\nAs organizations evaluate and adopt generative AI for their employees and customers, cybersecurity practitioners must assess the risks, governance, and controls for this evolving technology at a rapid pace. As security leaders working with the largest, most complex customers at Amazon Web Services (AWS), we’re regularly consulted on trends, best practices, and the rapidly evolving landscape of generative AI and the associated security and privacy implications. In that spirit, we’d like to share key strategies that you can use to accelerate your own generative AI security journey.\nThis post, the first in a series on securing generative AI, establishes a mental model that will help you approach the risk and security implications based on the type of generative AI workload you are deploying. We then highlight key considerations for security leaders and practitioners to prioritize when securing generative AI workloads. Follow-on posts will dive deep into developing generative AI solutions that meet customers’ security requirements, best practices for threat modeling generative AI applications, approaches for evaluating compliance and privacy considerations, and will explore ways to use generative AI to improve your own cybersecurity operations.\nWhere to start As with any emerging technology, a strong grounding in the foundations of that technology is critical to helping you understand the associated scopes, risks, security, and compliance requirements. To learn more about the foundations of generative AI, we recommend that you start by reading more about what generative AI is, its unique terminologies and nuances, and exploring examples of how organizations are using it to innovate for their customers.\nIf you’re just starting to explore or adopt generative AI, you might imagine that an entirely new security discipline will be required. While there are unique security considerations, the good news is that generative AI workloads are, at their core, another data-driven computing workload, and they inherit much of the same security regimen. The fact is, if you’ve invested in cloud cybersecurity best practices over the years and embraced prescriptive advice from sources like Steve’s top 10, the Security Pillar of the Well-Architected Framework, and the Well-Architected Machine Learning Lens, you’re well on your way!\nCore security disciplines, like identity and access management, data protection, privacy and compliance, application security, and threat modeling are still critically important for generative AI workloads, just as they are for any other workload. For example, if your generative AI application is accessing a database, you’ll need to know what the data classification of the database is, how to protect that data, how to monitor for threats, and how to manage access. But beyond emphasizing long-standing security practices, it’s crucial to understand the unique risks and additional security considerations that generative AI workloads bring. This post highlights several security factors, both new and familiar, for you to consider.\nWith that in mind, let’s discuss the first step: scoping.\nDetermine your scope Your organization has made the decision to move forward with a generative AI solution; now what do you do as a security leader or practitioner? As with any security effort, you must understand the scope of what you’re tasked with securing. Depending on your use case, you might choose a managed service where the service provider takes more responsibility for the management of the service and model, or you might choose to build your own service and model.\nLet’s look at how you might use various generative AI solutions in the AWS Cloud. At AWS, security is a top priority, and we believe providing customers with the right tool for the job is critical. For example, you can use the serverless, API-driven Amazon Bedrock with simple-to-consume, pre-trained foundation models (FMs) provided by AI21 Labs, Anthropic, Cohere, Meta, stability.ai, and Amazon Titan. Amazon SageMaker JumpStart provides you with additional flexibility while still using pre-trained FMs, helping you to accelerate your AI journey securely. You can also build and train your own models on Amazon SageMaker. Maybe you plan to use a consumer generative AI application through a web interface or API such as a chatbot or generative AI features embedded into a commercial enterprise application your organization has procured. Each of these service offerings has different infrastructure, software, access, and data models and, as such, will result in different security considerations. To establish consistency, we’ve grouped these service offerings into logical categorizations, which we’ve named scopes.\nIn order to help simplify your security scoping efforts, we’ve created a matrix that conveniently summarizes key security disciplines that you should consider, depending on which generative AI solution you select. We call this the Generative AI Security Scoping Matrix, shown in Figure 1.\nFigure 1. Generative AI Security Scoping Matrix\nThe first step is to determine which scope your use case fits into. The scopes are numbered 1–5, representing least ownership to greatest ownership.\nBuying generative AI:\nScope 1: Consumer app – Your business consumes a public third-party generative AI service, either at no-cost or paid. At this scope you don’t own or see the training data or the model, and you cannot modify or augment it. You invoke APIs or directly use the application according to the terms of service of the provider. Example: An employee interacts with a generative AI chat application to generate ideas for an upcoming marketing campaign.\nScope 2: Enterprise app – Your business uses a third-party enterprise application that has generative AI features embedded within, and a business relationship is established between your organization and the vendor. Example: You use a third-party enterprise scheduling application that has a generative AI capability embedded within to help draft meeting agendas. |\nBuilding generative AI:\nScope 3: Pre-trained models – Your business builds its own application using an existing third-party generative AI foundation model. You directly integrate it with your workload through an application programming interface (API).\nExample: You build an application to create a customer support chatbot that uses the Anthropic Claude foundation model through Amazon Bedrock APIs.\nScope 4: Fine-tuned models – Your business refines an existing third-party generative AI foundation model by fine-tuning it with data specific to your business, generating a new, enhanced model that’s specialized to your workload.\nExample: Using an API to access a foundation model, you build an application for your marketing teams that enables them to build marketing materials that are specific to your products and services.\nScope 5: Self-trained models – Your business builds and trains a generative AI model from scratch using data that you own or acquire. You own every aspect of the model.\nExample: Your business wants to create a model trained exclusively on deep, industry-specific data to license to companies in that industry, creating a completely novel LLM.\nIn the Generative AI Security Scoping Matrix, we identify five security disciplines that span the different types of generative AI solutions. The unique requirements of each security discipline can vary depending on the scope of the generative AI application. By determining which generative AI scope is being deployed, security teams can quickly prioritize focus and assess the scope of each security discipline.\nLet’s explore each security discipline and consider how scoping affects security requirements.\nGovernance and compliance – The policies, procedures, and reporting needed to empower the business while minimizing risk.\nLegal and privacy – The specific regulatory, legal, and privacy requirements for using or creating generative AI solutions.\nRisk management – Identification of potential threats to generative AI solutions and recommended mitigations.\nControls – The implementation of security controls that are used to mitigate risk.\nResilience – How to architect generative AI solutions to maintain availability and meet business SLAs.\nThroughout our Securing Generative AI blog series, we’ll be referring to the Generative AI Security Scoping Matrix to help you understand how various security requirements and recommendations can change depending on the scope of your AI deployment. We encourage you to adopt and reference the Generative AI Security Scoping Matrix in your own internal processes, such as procurement, evaluation, and security architecture scoping.\nWhat to prioritize Your workload is scoped and now you need to enable your business to move forward fast, yet securely. Let’s explore a few examples of opportunities you should prioritize.\nGovernance and compliance plus Legal and privacy\nFigure 2. Governance and compliance\nWith consumer off-the-shelf apps (Scope 1) and enterprise off-the-shelf apps (Scope 2), you must pay special attention to the terms of service, licensing, data sovereignty, and other legal disclosures. Outline important considerations regarding your organization’s data management requirements, and if your organization has legal and procurement departments, be sure to work closely with them. Assess how these requirements apply to a Scope 1 or 2 application. Data governance is critical, and an existing strong data governance strategy can be leveraged and extended to generative AI workloads. Outline your organization’s risk appetite and the security posture you want to achieve for Scope 1 and 2 applications and implement policies that specify that only appropriate data types and data classifications should be used. For example, you might choose to create a policy that prohibits the use of personal identifiable information (PII), confidential, or proprietary data when using Scope 1 applications.\nIf a third-party model has all the data and functionality that you need, Scope 1 and Scope 2 applications might fit your requirements. However, if it’s important to summarize, correlate, and parse through your own business data, generate new insights, or automate repetitive tasks, you’ll need to deploy an application from Scope 3, 4, or 5. For example, your organization might choose to use a pre-trained model (Scope 3). Maybe you want to take it a step further and create a version of a third-party model such as Amazon Titan with your organization’s data included, known as fine-tuning (Scope 4). Or you might create an entirely new first-party model from scratch, trained with data you supply (Scope 5).\nIn Scopes 3, 4, and 5, your data can be used in the training or fine-tuning of the model, or as part of the output. You must understand the data classification and data type of the assets the solution will have access to. Scope 3 solutions might use a filtering mechanism on data provided through Retrieval Augmented Generation (RAG) with the help from Agents for Amazon Bedrock, for example, as an input to a prompt. RAG offers you an alternative to training or fine-tuning by querying your data as part of the prompt. This then augments the context for the LLM to provide a completion and response that can use your business data as part of the response, rather than directly embedding your data in the model itself through fine-tuning or training. See Figure 3 for an example data flow diagram demonstrating how customer data could be used in a generative AI prompt and response through RAG.\nFigure 3. Retrieval Augmented Generation (RAG)\nIn scopes 4 and 5, on the other hand, you must classify the modified model for the most sensitive level of data classification used to fine-tune or train the model. Your model would then mirror the data classification on the data it was trained against. For example, if you supply PII in the fine-tuning or training of a model, then the new model will contain PII. Currently, there are no mechanisms for easily filtering the model’s output based on authorization, and a user could potentially retrieve data they wouldn’t otherwise be authorized to see. Consider this a key takeaway; your application can be built around your model to implement filtering controls on your business data as part of a RAG data flow, which can provide additional data security granularity without placing your sensitive data directly within the model.\nFigure 4. Legal and privacy\nFrom a legal perspective, it’s important to understand both the service provider’s end-user license agreement (EULA), terms of services (TOS), and any other contractual agreements necessary to use their service across Scopes 1 through 4. For Scope 5, your legal teams should provide their own contractual terms of service for any external use of your models. Also, for Scope 3 and Scope 4, be sure to validate both the service provider’s legal terms for the use of their service, as well as the model provider’s legal terms for the use of their model within that service.\nAdditionally, consider the privacy concerns if the European Union’s General Data Protection Regulation (GDPR) “right to erasure” or “right to be forgotten” requirements are applicable to your business. Carefully consider the impact of training or fine-tuning your models with data that you might need to delete upon request. The only fully effective way to remove data from a model is to delete the data from the training set and train a new version of the model. This isn’t practical when the data deletion is a fraction of the total training data and can be very costly depending on the size of your model.\nRisk management Figure 5. Risk management\nWhile AI-enabled applications can act, look, and feel like non-AI-enabled applications, the free-form nature of interacting with an LLM mandates additional scrutiny and guardrails. It is important to identify what risks apply to your generative AI workloads, and how to begin to mitigate them.\nThere are many ways to identify risks, but two common mechanisms are risk assessments and threat modeling. For Scopes 1 and 2, you’re assessing the risk of the third-party providers to understand the risks that might originate in their service, and how they mitigate or manage the risks they’re responsible for. Likewise, you must understand what your risk management responsibilities are as a consumer of that service.\nFor Scopes 3, 4, and 5—implement threat modeling—while we will dive deep into specific threats and how to threat-model generative AI applications in a future blog post, let’s give an example of a threat unique to LLMs. Threat actors might use a technique such as prompt injection: a carefully crafted input that causes an LLM to respond in unexpected or undesired ways. This threat can be used to extract features (features are characteristics or properties of data used to train a machine learning (ML) model), defame, gain access to internal systems, and more. In recent months, NIST, MITRE, and OWASP have published guidance for securing AI and LLM solutions. In both the MITRE and OWASP published approaches, prompt injection (model evasion) is the first threat listed. Prompt injection threats might sound new, but will be familiar to many cybersecurity professionals. It’s essentially an evolution of injection attacks, such as SQL injection, JSON or XML injection, or command-line injection, that many practitioners are accustomed to addressing.\nEmerging threat vectors for generative AI workloads create a new frontier for threat modeling and overall risk management practices. As mentioned, your existing cybersecurity practices will apply here as well, but you must adapt to account for unique threats in this space. Partnering deeply with development teams and other key stakeholders who are creating generative AI applications within your organization will be required to understand the nuances, adequately model the threats, and define best practices.\nControls Controls help us enforce compliance, policy, and security requirements in order to mitigate risk. Let’s dive into an example of a prioritized security control: identity and access management. To set some context, during inference (the process of a model generating an output, based on an input) first- or third-party foundation models (Scopes 3–5) are immutable. The API to a model accepts an input and returns an output. Models are versioned and, after release, are static. On its own, the model itself is incapable of storing new data, adjusting results over time, or incorporating external data sources directly. Without the intervention of data processing capabilities that reside outside of the model, the model will not store new data or mutate.\nBoth modern databases and foundation models have a notion of using the identity of the entity making a query. Traditional databases can have table-level, row-level, column-level, or even element-level security controls. Foundation models, on the other hand, don’t currently allow for fine-grained access to specific embeddings they might contain. In LLMs, embeddings are the mathematical representations created by the model during training to represent each object—such as words, sounds, and graphics—and help describe an object’s context and relationship to other objects. An entity is either permitted to access the full model and the inference it produces or nothing at all. It cannot restrict access at the level of specific embeddings in a vector database. In other words, with today’s technology, when you grant an entity access directly to a model, you are granting it permission to all the data that model was trained on. When accessed, information flows in two directions: prompts and contexts flow from the user through the application to the model, and a completion returns from the model back through the application providing an inference response to the user. When you authorize access to a model, you’re implicitly authorizing both of these data flows to occur, and either or both of these data flows might contain confidential data.\nFor example, imagine your business has built an application on top of Amazon Bedrock at Scope 4, where you’ve fine-tuned a foundation model, or Scope 5 where you’ve trained a model on your own business data. An AWS Identity and Access Management (IAM) policy grants your application permissions to invoke a specific model. The policy cannot limit access to subsets of data within the model. For IAM, when interacting with a model directly, you’re limited to model access.\n\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: {\r\u0026quot;Sid\u0026quot;: \u0026quot;AllowInference\u0026quot;,\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;bedrock:InvokeModel\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:bedrock:*::\u0026lt;foundation-model\u0026gt;/\r\u0026lt;model-id-of-model-to-allow\u0026gt;\r}\rWhat could you do to implement least privilege in this case? In most scenarios, an application layer will invoke the Amazon Bedrock endpoint to interact with a model. This front-end application can use an identity solution, such as Amazon Cognito or AWS IAM Identity Center, to authenticate and authorize users, and limit specific actions and access to certain data accordingly based on roles, attributes, and user communities. For example, the application could select a model based on the authorization of the user. Or perhaps your application uses RAG by querying external data sources to provide just-in-time data for generative AI responses, using services such as Amazon Kendra or Amazon OpenSearch Serverless. In that case, you would use an authorization layer to filter access to specific content based on the role and entitlements of the user. As you can see, identity and access management principles are the same as any other application your organization develops, but you must account for the unique capabilities and architectural considerations of your generative AI workloads.\nResilience Finally, availability is a key component of security as called out in the C.I.A. triad. Building resilient applications is critical to meeting your organization’s availability and business continuity requirements. For Scope 1 and 2, you should understand how the provider’s availability aligns to your organization’s needs and expectations. Carefully consider how disruptions might impact your business should the underlying model, API, or presentation layer become unavailable. Additionally, consider how complex prompts and completions might impact usage quotas, or what billing impacts the application might have.\nFor Scopes 3, 4, and 5, make sure that you set appropriate timeouts to account for complex prompts and completions. You might also want to look at prompt input size for allocated character limits defined by your model. Also consider existing best practices for resilient designs such as backoff and retries and circuit breaker patterns to achieve the desired user experience. When using vector databases, having a high availability configuration and disaster recovery plan is recommended to be resilient against different failure modes.\nInstance flexibility for both inference and training model pipelines are important architectural considerations in addition to potentially reserving or pre-provisioning compute for highly critical workloads. When using managed services like Amazon Bedrock or SageMaker, you must validate AWS Region availability and feature parity when implementing a multi-Region deployment strategy. Similarly, for multi-Region support of Scope 4 and 5 workloads, you must account for the availability of your fine-tuning or training data across Regions. If you use SageMaker to train a model in Scope 5, use checkpoints to save progress as you train your model. This will allow you to resume training from the last saved checkpoint if necessary.\nBe sure to review and implement existing application resilience best practices established in the AWS Resilience Hub and within the Reliability Pillar and Operational Excellence Pillar of the Well Architected Framework.\nConclusion In this post, we outlined how well-established cloud security principles provide a solid foundation for securing generative AI solutions. While you will use many existing security practices and patterns, you must also learn the fundamentals of generative AI and the unique threats and security considerations that must be addressed. Use the Generative AI Security Scoping Matrix to help determine the scope of your generative AI workloads and the associated security dimensions that apply. With your scope determined, you can then prioritize solving for your critical security requirements to enable the secure use of generative AI workloads by your business.\nWant to dive deeper into additional areas of generative AI security? Check out the other posts in the Securing Generative AI series:\nPart 1 – Securing Generative AI: An introduction to the Generative AI Security Scoping Matrix (this post)\nPart 2 – Designing generative AI workloads for resilience\nPart 3 – Securing Generative AI: Applying relevant security controls\nPart 4 – Securing generative AI: data, compliance, and privacy considerations\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Lê Vũ Tiến Dũng\nPhone Number: 0911434175\nEmail: tamkg1102@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Get familiar with AWS interface and core concepts. Learn how to create and configure an AWS Free Tier account, including security (MFA) and cost management. Understand billing alerts and basic AWS Budgets usage. Master AWS IAM for access management and security best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Register and activate AWS Free Tier account - Enable MFA on root account - Configure billing alerts 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/vi/ 2 - Create AWS Budgets - Setup alert emails for cost thresholds - Explore Cost Explorer 10/09/2025 10/09/2025 https://000007.awsstudygroup.com/vi/ 3 - Explore AWS Support plans: + Basic + Developer + Business - Understand support response times 11/09/2025 11/09/2025 https://000009.awsstudygroup.com/vi/1-support-plans/ 4 - Overview of AWS IAM fundamentals - Create IAM users and groups - Learn about IAM policies 12/09/2025 12/09/2025 https://000002.awsstudygroup.com/vi/ 5 - Mini practice: + Setup MFA for IAM users + Test IAM permissions + Create an S3 bucket + Delete the S3 bucket to check Free Tier billing 13/09/2025 13/09/2025 https://000057.awsstudygroup.com/vi/ Week 1 Achievements: Successfully created and secured AWS Free Tier account with MFA. Learned to configure billing alerts and monitor budgets. Gained familiarity with AWS Support plans and console interface. Mastered AWS IAM fundamentals: users, groups, and policies. Practiced basic S3 operations and understood their impact on Free Tier limits. Summary: By the end of Week 1, I was able to:\nConfidently navigate AWS Management Console. Securely manage account settings including MFA and billing. Create budgets and alerts to monitor Free Tier usage. Understand and implement AWS IAM for secure access management. Perform basic S3 operations safely. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will find my comprehensive 12-week AWS learning journey worklog. This program was designed to build foundational to advanced AWS cloud computing skills through hands-on practice with First Cloud Journey (FCJ) workshops.\nHow did I complete it? I followed a structured learning path from September 9, 2025, focusing on practical labs and real-world scenarios. Each week consisted of 5 days (Monday to Friday) of intensive learning, with 10-15 hours dedicated to hands-on practice, documentation reading, and project implementation.\nProgram Duration: 12 weeks (3 months)\nLearning Approach:\n60% hands-on practice with AWS services 30% reading AWS documentation and FCJ workshop materials 10% completing exercises and reviewing best practices What did I accomplish?\nThroughout this 12-week program, I progressively built expertise across 25+ AWS services, starting from account setup to deploying production-ready, highly available applications. Below is the weekly breakdown:\nWeek 1: AWS Fundamentals - Account Setup, Billing, Support, and IAM\nWeek 2: AWS Networking - VPC, Subnets, Security Groups, and AWS CLI\nWeek 3: Compute Services - EC2, IAM Roles, and AWS Cloud9\nWeek 4: Storage and Simplified Computing - S3 Static Hosting, Lightsail, and Containers\nWeek 5: Database Fundamentals - Amazon RDS and DynamoDB\nWeek 6: Database Optimization - ElastiCache and 3-Tier Architecture\nWeek 7: Scaling and Monitoring - Auto Scaling and CloudWatch\nWeek 8: Global Distribution - Route 53, CloudFront, and Lambda@Edge\nWeek 9: Windows on AWS - Windows Server EC2 and Managed Microsoft AD\nWeek 10: Hybrid Directory Services - AD Integration and Enterprise Authentication\nWeek 11: High Availability - Load Balancing, Multi-Tier Apps, and Disaster Recovery\nWeek 12: Capstone Project - Comprehensive Application Deployment and Certification Prep\nKey Achievements:\nMastered 15+ AWS services across compute, storage, database, networking, and security Built production-ready architectures with high availability and fault tolerance Implemented cost optimization and monitoring strategies Gained hands-on experience with Windows workloads and enterprise directory services Completed comprehensive capstone project integrating all learned services Next Steps:\nPursue AWS Certified Solutions Architect - Associate certification Continue exploring advanced AWS services Contribute to AWS community and open-source projects "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "Workshop Objective In this workshop, you will learn how to deploy a backend system for a game, including:\nCognito: login system with Username/Password and OAuth (Google). DynamoDB: storing player profiles, progress, and scores. S3: storing player avatars, only allowing uploads via pre-signed URL. Lambda: backend APIs using standard ZIP Lambdas and Container Lambda (OpenCV). API Gateway REST \u0026amp; WebSocket: serving REST routes and realtime leaderboard. CI/CD (CodePipeline + CodeBuild): automatically build container and deploy Lambda. IAM Roles: permissions for Lambda and CodeBuild. CloudWatch: basic logging, billing alarm, and error alarm. You will deploy each AWS service step by step, test using CLI or console, and finally collect all necessary information to hand over to FE and BE teams.\nLab Sections Cognito – Login System DynamoDB – Main Database S3 – Store Player Avatars ECR – Avatar Processing Container Lambda – Backend APIs API Gateway REST API Gateway WebSocket CI/CD – Backend IAM Roles Logging \u0026amp; Monitoring WAF (Optional) Prerequisites AWS account with sufficient IAM permissions to create and configure the services above. Use ap-southeast-2 region for this workshop. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “Kick-off AWS FCJ Workforce -FPTU OJT FALL 2025” Event Objectives Build a new generation of high-quality AWS Builders for Vietnam. Equip students with hands-on skills in Cloud, DevOps, AI/ML, Security, and Data \u0026amp; Analytics. Connect students with the AWS Study Group community of 47,000+ members and AWS partner companies. Deliver strategic career orientation covering specialized fields like Cloud Computing, DevOps, and GenAI. Speakers Nguyễn Gia Hưng – Head of Solutions Architect, AWS Vietnam Đỗ Huy Thắng – DevOps Lead, VNG Danh Hoàng Hiếu Nghị – GenAI Engineer, Renova Bùi Hồ Linh Nhi – AI Engineer, SoftwareOne Phạm Nguyễn Hải Anh – Cloud Engineer, G-Asia Pacific Nguyễn Đồng Thanh Hiệp – Principal Cloud Engineer, G-Asia Pacific Introduce AWS program Sharing what we have to do when we join AWS program Speak about devOps, advantage and the other services we are about to learn Introduce exccelent member in the AWS and things they have don AWS First Cloud Journey \u0026amp; Future Direction The AWS First Cloud Journey marks the beginning of a transformative path toward building cloud expertise and innovation. It empowers learners and organizations to explore the core foundations of AWS services — from cloud computing, networking, and storage to advanced topics like DevOps, AI/ML, and data analytics. This journey not only provides practical, hands-on experience but also cultivates a mindset of agility, scalability, and continuous improvement. Looking toward the future direction, AWS aims to foster a new generation of cloud professionals who can drive digital transformation, implement sustainable cloud solutions, and harness cutting-edge technologies to shape the future of Vietnam’s digital economy. DevOps \u0026amp; Future Career Bridge between Development and Operations: Connects software development and IT operations to enhance collaboration, speed, and reliability. Culture of Continuous Improvement: Focuses on automation, testing, monitoring, and feedback for faster and higher-quality delivery. High Demand for DevOps Professionals: Cloud adoption drives strong demand for DevOps experts across all industries. Key Future Skills: Cloud platforms (AWS, Azure, GCP) → CI/CD pipelines → containerization (Docker, Kubernetes) → infrastructure as code (Terraform, CloudFormation). Career Growth Opportunities: Roles like DevOps Engineer, Cloud Architect, SRE, and Platform Engineer will continue expanding globally. Mindset for Success: Agility, collaboration, problem-solving, and continuous learning are essential for thriving in the DevOps era. From First Cloud Journey to GenAI Engineer First Cloud Journey: The starting point to explore cloud computing, mastering the fundamentals of AWS services and architecture. Building Cloud Foundations: Learn key domains such as networking, storage, security, and compute to understand how modern cloud systems operate. DevOps Integration: Adopt automation, CI/CD pipelines, and Infrastructure as Code to enable faster and more reliable deployments. Data \u0026amp; Analytics Skills: Gain the ability to collect, process, and visualize data for informed business decisions. AI/ML Advancement: Move from cloud operations to applying machine learning and artificial intelligence using AWS AI services. GenAI Engineer Path: Leverage cloud, DevOps, and AI expertise to design, deploy, and optimize generative AI applications for real-world innovation. A Day in the Life of a Cloud Engineer Morning Check-ins: Review system dashboards, monitor cloud resources, and ensure all services are running smoothly. Infrastructure Management: Use Infrastructure as Code (IaC) tools like Terraform or CloudFormation to deploy and manage environments. Security \u0026amp; Compliance: Apply IAM policies, monitor access logs, and ensure systems meet security best practices. Automation \u0026amp; DevOps Tasks: Build CI/CD pipelines, automate deployments, and optimize performance for scalability. Collaboration \u0026amp; Problem-Solving: Work with developers, DevOps, and data teams to troubleshoot and improve cloud solutions. Learning \u0026amp; Innovation: Stay updated with the latest AWS services, explore AI/ML integrations, and continuously improve cloud architecture skills. Event Experience Attending the “Kick-off AWS FCJ Workforce -FPTU OJT FALL 2025” workshop was extremely valuable, giving me a comprehensive view of people have been studied and exprienced. We also know the others services in AWS and witnessed many project that was created by many exprienced people and could hear a story of the team AWS their journey to become the best developer.\nLessons learned Know that we are going to do in AWS company. Make Friends and you could learn from your mentor. Got motivation to working in AWS. Overall, the event just about introduce about the company , people and other services . I also got motivation to move forward and learn as best as i can of AWS services.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand AWS networking fundamentals with Amazon VPC. Learn about VPC components: Subnets, Route Tables, Internet Gateways, Security Groups, and NACLs. Practice advanced networking concepts through AWS networking workshop. Master AWS CLI for command-line infrastructure management. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Amazon VPC basics - Understand VPC CIDR blocks - Create default VPC 16/09/2025 16/09/2025 https://000003.awsstudygroup.com/vi/ 2 - Create Subnets (public and private) - Configure Route Tables - Setup Internet Gateway 17/09/2025 17/09/2025 https://000004.awsstudygroup.com/vi/2-prerequiste/2.2-createvpcwindowsinstance/ 3 - Learn about Security Groups - Learn about Network ACLs - Understand differences between Security Groups and NACLs 18/09/2025 18/09/2025 https://000003.awsstudygroup.com/vi/2-firewallinvpc/ 4 - Complete AWS Networking Workshop - Practice VPC Peering - Learn about NAT Gateway 19/09/2025 19/09/2025 https://000003.awsstudygroup.com/vi/4-createec2server/4.3-natgateway/ 5 - Install and configure AWS CLI - Learn basic CLI commands - Mini practice: + Create VPC using CLI + List and describe VPC resources + Delete VPC using CLI 20/09/2025 20/09/2025 https://000011.awsstudygroup.com/vi/ Week 2 Achievements: Successfully understood Amazon VPC architecture and components. Learned to create and configure Subnets, Route Tables, and Internet Gateways. Mastered the difference between Security Groups and Network ACLs. Completed AWS Networking Workshop with hands-on VPC scenarios. Installed and configured AWS CLI for command-line operations. Summary: By the end of Week 2, I was able to:\nDesign and create custom VPC with public and private subnets. Configure routing and internet connectivity for VPC resources. Implement network security using Security Groups and NACLs. Use AWS CLI to manage AWS resources from the command line. Understand advanced networking concepts like VPC Peering and NAT Gateway. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "IAM Permissions Add the following IAM policy to your user account to deploy and manage the game backend project:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;GameProjectPermissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;dynamodb:*\u0026#34;, \u0026#34;s3:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;codebuild:*\u0026#34;, \u0026#34;codepipeline:*\u0026#34;, \u0026#34;logs:*\u0026#34;, \u0026#34;sns:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Serverless Multiplayer Game Backend A Scalable AWS Solution for Real-Time Gaming \u0026amp; AI Avatar Processing\n1. Executive Summary This project aims to build a robust, serverless backend infrastructure for a multiplayer Unity game. The system separates responsibilities across DevOps, Frontend (Unity), and Backend teams. AWS services are used to handle:\nUser Authentication → Amazon Cognito Gameplay Logic → AWS Lambda Real-time Leaderboards → API Gateway WebSocket + DynamoDB Streams AI Avatar Processing → Container-based Lambda (OpenCV/MediaPipe) The architecture provides high availability, CI/CD automation, and seamless WebGL integration deployed on itch.io or CloudFront. 2. Problem Statement What’s the Problem? Multiplayer games require complex backend features such as authentication, real-time communication, and persistence. Traditional monolithic servers are costly, difficult to maintain, and do not scale effectively with player spikes. The game also requires AI-driven avatar transformation, which is computationally intensive.\nThe Solution A fully serverless AWS architecture:\nAuthentication: Amazon Cognito (User Pools + Hosted UI) Logic \u0026amp; Compute: Lambda (zip + container images from ECR) Real-time Interaction: WebSocket API + DynamoDB Streams Storage: Amazon S3 for avatars/assets Benefits \u0026amp; ROI Cost Efficiency: Pay-as-you-go (Lambda + DynamoDB) Scalability: Auto-scales for player spikes Automation: CI/CD pipeline for instant deployments 3. Solution Architecture The system follows an event-driven microservices design. Unity interacts with AWS via REST (scores, shop) and WebSocket (live leaderboard). AI processing is handled by containerized Lambda functions.\nAWS Services Used Amazon Cognito – User Pools, Hosted UI API Gateway (REST + WebSocket) AWS Lambda (Zip + Container Image) Amazon DynamoDB + Streams Amazon S3 Amazon ECR Integration Complexity: High impact / Medium probability Latency Issues: Medium impact / Low probability Unexpected Costs: Low impact / Low probability Mitigation Strategies Use Postman Mock Server for FE development Placeholder leaderboard/task logic CloudWatch Logs + alarms (errors \u0026gt; 10/min) 4. Technical Implementation Implementation Phases Infrastructure Setup (DevOps) – Cognito, DynamoDB, S3, API Gateway Backend Skeleton (BE) – API specs, Postman, Lambda base code Login Integration (FE) – Unity AuthManager Wiring \u0026amp; Streams (DevOps) – API → Lambda, enable Streams Gameplay Integration (FE) – DataManager → REST APIs End-to-End Testing – Login, Shop, Leaderboards, Avatar Deployment – WebGL build + redirect URLs Technical Requirements Frontend: Unity (C#) – AwsConfig, AuthManager, DataManager, RealtimeManager Backend: Node.js/Python for Lambdas, Docker for AI containers DevOps: IAM roles, CloudFormation (optional), WAF (optional) 5. Timeline \u0026amp; Milestones Phase 1: Foundation (Days 1–3) DevOps sets up Cognito, DynamoDB, S3, API Gateway. Phase 2: Logic Development (Days 3–8) Backend builds Lambda functions + Avatar AI Container Frontend builds Login Phase 3: Integration (Days 8–12) DevOps connects Streams Frontend integrates APIs Phase 4: Testing \u0026amp; Launch (Days 13–15) Test real-time leaderboard + avatar pipeline Deploy WebGL 6. Budget Estimation (Refer to AWS Pricing Calculator)\nInfrastructure Costs Lambda: Mostly Free Tier DynamoDB: Free Tier (25GB) S3: ~$0.023/GB CloudWatch: ~$0.5–1/month ECR: ~$0.10/GB Total Estimated Cost: \u0026lt; $5/month during development. 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Understand Amazon EC2 fundamentals and instance types. Learn to launch and manage EC2 instances in VPC. Master IAM Roles for EC2 for secure application permissions. Get familiar with AWS Cloud9 IDE for cloud-based development. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Amazon EC2 basics - Understand EC2 instance types - Learn about AMIs (Amazon Machine Images) 23/09/2025 23/09/2025 https://000003.awsstudygroup.com/vi/4-createec2server/ 2 - Launch first EC2 instance - Configure Security Groups for EC2 - Connect to EC2 via SSH 24/09/2025 24/09/2025 https://000003.awsstudygroup.com/vi/4-createec2server/4.1-createec2/ 3 - Learn about IAM Roles for EC2 - Create IAM role with S3 access - Attach IAM role to EC2 instance 25/09/2025 25/09/2025 https://000048.awsstudygroup.com/vi/ 4 - Setup AWS Cloud9 environment - Explore Cloud9 IDE features - Write and run simple applications in Cloud9 26/09/2025 26/09/2025 https://000049.awsstudygroup.com/vi/ 5 - Mini practice: + Launch EC2 with IAM role + Access S3 from EC2 using IAM role + Deploy simple web app using Cloud9 + Stop and terminate EC2 instances 27/09/2025 27/09/2025 AWS Best Practices, FCJ Workshop Week 3 Achievements: Successfully understood Amazon EC2 architecture and instance types. Learned to launch and connect to EC2 instances via SSH. Mastered IAM Roles for EC2 for secure AWS service access without credentials. Setup and used AWS Cloud9 for cloud-based development. Practiced EC2 lifecycle management: launch, configure, stop, and terminate. Summary: By the end of Week 3, I was able to:\nLaunch and configure EC2 instances in custom VPC. Securely manage EC2 permissions using IAM Roles instead of access keys. Connect to EC2 instances and perform basic server administration. Use AWS Cloud9 IDE for developing and testing applications in the cloud. Understand EC2 pricing and how to avoid unnecessary charges. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Securing generative AI: An introduction to the Generative AI Security Scoping Matrix In this blog, we outlined how well-established cloud security principles provide a solid foundation for securing generative AI solutions. While you will use many existing security practices and patterns, you must also learn the fundamentals of generative AI and the unique threats and security considerations that must be addressed. Use the Generative AI Security Scoping Matrix to help determine the scope of your generative AI workloads and the associated security dimensions that apply. With your scope determined, you can then prioritize solving for your critical security requirements to enable the secure use of generative AI workloads by your business.\nBlog 2 - Amazon OpenSearch Service’s vector database capabilities explained In this blog, we describe how OpenSearch supports a variety of engines, algorithms, and distance measures that you can employ to build the right solution. OpenSearch provides a scalable engine that can support vector search at low latency and up to billions of vectors. With OpenSearch and its vector DB capabilities, your users can find that 8-foot-blue couch easily, and relax by a cozy fire.. Whether you’re building a generative AI solution, searching rich media and audio, or bringing more semantic search to your existing search-based application, OpenSearch is a capable vector database.\nBlog 3 - How Smartsheet enhances recommendations using Amazon Neptune and Knowledge Graphs In this blog, we describe the Smartsheet Knowledge Graph, built in partnership between Smartsheet and AWS. The Smartsheet Knowledge Graph is a unified data model connecting people, content, and work in Smartsheet, representing how users interact with assets, content, and their collaborators. Simply put, it is a professional network tailored for each user within Smartsheet. Capturing this collaboration between users and their work helps unlock personalization, insights, and recommendations for each user.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn Amazon S3 for static website hosting and object storage. Understand Amazon Lightsail for simplified cloud computing. Master container deployment with Amazon Lightsail Containers. Practice deploying real-world applications using simplified AWS services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Amazon S3 fundamentals - Understand S3 bucket policies - Configure S3 for static website hosting 30/09/2025 30/09/2025 https://000057.awsstudygroup.com/vi/ 2 - Practice S3 static hosting: - Upload HTML/CSS/JS files - Configure bucket for public access - Test website via S3 URL 01/10/2025 01/10/2025 https://000057.awsstudygroup.com/vi/2-prerequiste/2.2-uploaddata/ 3 - Learn about Amazon Lightsail - Understand Lightsail vs EC2 differences - Create Lightsail instance 02/10/2025 02/10/2025 https://000045.awsstudygroup.com/vi/ 4 - Learn Lightsail Containers - Understand container basics - Deploy first container to Lightsail 03/10/2025 03/10/2025 https://000046.awsstudygroup.com/vi/ 5 - Mini practice: + Deploy complete static website on S3 + Deploy containerized app on Lightsail + Compare costs: S3 vs Lightsail vs EC2 + Clean up resources 04/10/2025 04/10/2025 https://000057.awsstudygroup.com/vi/3-staticwebsite/ Week 4 Achievements: Successfully hosted static websites using Amazon S3. Learned to configure S3 bucket policies for public access. Mastered Amazon Lightsail for simplified cloud deployments. Deployed containerized applications using Lightsail Containers. Understood cost differences between S3, Lightsail, and EC2 for various use cases. Summary: By the end of Week 4, I was able to:\nHost and manage static websites on Amazon S3 with custom domain support. Configure S3 bucket policies and permissions for web hosting. Use Amazon Lightsail for quick and simple application deployments. Deploy and manage containerized applications using Lightsail Containers. Make informed decisions on choosing between S3, Lightsail, and EC2 based on requirements. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in total 4 events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments in my life.\nEvent 1 Event Name: Kick-off AWS FCJ Workforce -FPTU OJT FALL 2025\nDate \u0026amp; Time: 09:00, August 8, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: ​DevOps with AWS\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Understand Amazon RDS fundamentals for relational databases. Learn to create and manage RDS instances with proper configuration. Master Amazon DynamoDB for NoSQL database solutions. Compare relational vs NoSQL databases and their use cases. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Amazon RDS basics - Understand RDS database engines (MySQL, PostgreSQL, etc.) - Learn about Multi-AZ and Read Replicas 07/10/2025 07/10/2025 https://000005.awsstudygroup.com/vi/ 2 - Create RDS instance - Configure security groups for RDS - Connect to RDS from EC2 instance 08/10/2025 08/10/2025 https://000005.awsstudygroup.com/vi/4-create-rds/ 3 - Learn RDS backups and snapshots - Practice automated backups - Restore RDS from snapshot 09/10/2025 09/10/2025 https://000005.awsstudygroup.com/vi/6-backup/ 4 - Learn Amazon DynamoDB fundamentals - Understand NoSQL concepts - Create DynamoDB tables with partition and sort keys 10/10/2025 10/10/2025 https://000060.awsstudygroup.com/vi/ 5 - Mini practice: + Create RDS MySQL database + Create DynamoDB table + Perform CRUD operations on both databases + Compare RDS vs DynamoDB use cases 11/10/2025 11/10/2025 https://000005.awsstudygroup.com/vi/5-deploy-app/ Week 5 Achievements: Successfully understood Amazon RDS architecture and database engines. Learned to create and configure RDS instances with proper security. Mastered RDS backup and restore operations. Understood Amazon DynamoDB and NoSQL database concepts. Practiced CRUD operations on both relational and NoSQL databases. Summary: By the end of Week 5, I was able to:\nCreate and manage RDS instances with Multi-AZ deployment. Configure security groups and connect applications to RDS databases. Implement automated backup strategies and restore from snapshots. Design and create DynamoDB tables with appropriate partition keys. Choose between RDS and DynamoDB based on application requirements. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Understand Amazon ElastiCache for in-memory caching solutions. Learn to implement caching strategies to improve application performance. Practice integrating ElastiCache with applications and databases. Review and consolidate database knowledge from Week 5 and 6. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Amazon ElastiCache basics - Understand Redis vs Memcached - Learn caching strategies and patterns 14/10/2025 14/10/2025 https://000061.awsstudygroup.com/vi/1-introduce/ 2 - Create ElastiCache cluster (Redis) - Configure security groups for ElastiCache - Connect to ElastiCache from EC2 15/10/2025 15/10/2025 https://000061.awsstudygroup.com/vi/3-amazonelasticacheforredis/3.4-grantaccesstocluster/ 3 - Implement caching layer for database queries - Practice cache invalidation strategies - Monitor cache hit/miss ratios 16/10/2025 16/10/2025 AWS Caching Best Practices, FCJ Workshop 4 - Integration practice: - Build application with RDS + ElastiCache - Implement session storage with Redis - Test performance improvements 17/10/2025 17/10/2025 AWS Architecture Patterns, FCJ Workshop 5 - Mini practice: + Deploy complete 3-tier app (EC2 + RDS + ElastiCache) + Measure performance with and without cache + Review database architectures + Clean up all database resources 18/10/2025 18/10/2025 AWS Well-Architected Framework, FCJ Workshop Week 6 Achievements: Successfully understood Amazon ElastiCache and in-memory caching concepts. Learned the differences between Redis and Memcached engines. Mastered implementing caching layers to improve application performance. Integrated ElastiCache with RDS for optimized database queries. Built complete 3-tier application architecture with caching. Summary: By the end of Week 6, I was able to:\nCreate and configure ElastiCache clusters for application caching. Implement effective caching strategies to reduce database load. Integrate ElastiCache with applications for session management and query caching. Measure and analyze performance improvements using caching. Design complete database architectures combining RDS, DynamoDB, and ElastiCache. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Understand EC2 Auto Scaling for automatic capacity management. Learn to create Auto Scaling Groups with launch templates. Master Amazon CloudWatch for monitoring and alerting. Implement scaling policies based on CloudWatch metrics. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn EC2 Auto Scaling fundamentals - Understand scaling policies (target tracking, step, scheduled) - Learn about Launch Templates 21/10/2025 21/10/2025 https://000006.awsstudygroup.com/vi/ 2 - Create Launch Template - Create Auto Scaling Group - Configure desired, minimum, and maximum capacity 22/10/2025 22/10/2025 https://000006.awsstudygroup.com/vi/6-create-auto-scaling-group/ 3 - Learn Amazon CloudWatch basics - Understand CloudWatch metrics and namespaces - Create CloudWatch dashboards 23/10/2025 23/10/2025 https://000008.awsstudygroup.com/vi/ 4 - Configure CloudWatch Alarms - Setup SNS notifications - Integrate alarms with Auto Scaling policies 24/10/2025 24/10/2025 https://000008.awsstudygroup.com/vi/5-cloud-watch-alarm/ 5 - Mini practice: + Create Auto Scaling Group with scaling policies + Setup CloudWatch monitoring and alarms + Test scaling by simulating load + Monitor scaling activities in CloudWatch 25/10/2025 25/10/2025 AWS Monitoring Best Practices, FCJ Workshop Week 7 Achievements: Successfully understood EC2 Auto Scaling and its scaling policies. Learned to create Launch Templates and Auto Scaling Groups. Mastered Amazon CloudWatch for monitoring AWS resources. Configured CloudWatch Alarms with SNS notifications. Implemented automatic scaling based on CloudWatch metrics. Summary: By the end of Week 7, I was able to:\nCreate and configure Auto Scaling Groups for automatic capacity management. Design scaling policies based on application requirements (CPU, memory, custom metrics). Build CloudWatch dashboards for monitoring multiple AWS resources. Set up CloudWatch alarms to trigger Auto Scaling and send notifications. Test and validate Auto Scaling behavior under different load conditions. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Understand Amazon Route 53 for DNS management. Learn Amazon CloudFront for content delivery and distribution. Master Lambda@Edge for edge computing with CloudFront. Implement global content distribution with low latency. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Amazon Route 53 basics - Understand DNS concepts and routing policies - Create hosted zone and record sets 28/10/2025 28/10/2025 https://000010.awsstudygroup.com/vi/ 2 - Configure Route 53 routing policies - Practice simple, weighted, and failover routing - Setup health checks for DNS failover 29/10/2025 29/10/2025 https://000010.awsstudygroup.com/vi/2-prerequiste/ 3 - Learn Amazon CloudFront fundamentals - Understand CDN concepts and edge locations - Create CloudFront distribution 30/10/2025 30/10/2025 https://000094.awsstudygroup.com/vi/ 4 - Configure CloudFront with S3 origin - Setup custom domain with Route 53 - Implement SSL/TLS certificates with ACM 31/10/2025 31/10/2025 https://000094.awsstudygroup.com/vi/1.-cloud-front-v%E1%BB%9Bi-s3/1.3-c%E1%BA%A5u-h%C3%ACnh-amazon-cloudfront/ 5 - Learn Lambda@Edge - Mini practice: + Create CloudFront distribution with custom domain + Deploy Lambda@Edge function + Test edge computing scenarios + Monitor CloudFront performance 01/11/2025 01/11/2025 https://000130.awsstudygroup.com/vi/ Week 8 Achievements: Successfully understood Amazon Route 53 and DNS management. Learned to configure routing policies for high availability and failover. Mastered Amazon CloudFront for global content distribution. Implemented Lambda@Edge for edge computing capabilities. Integrated Route 53, CloudFront, and Lambda@Edge for complete CDN solution. Summary: By the end of Week 8, I was able to:\nManage DNS with Route 53 and configure various routing policies. Create CloudFront distributions for fast content delivery worldwide. Implement SSL/TLS certificates using AWS Certificate Manager. Deploy Lambda@Edge functions for request/response manipulation at edge locations. Design global architectures with low latency using Route 53 and CloudFront. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Understand Windows applications on AWS. Learn to deploy and manage Windows Server on EC2. Master AWS Managed Microsoft AD for directory services. Integrate Windows workloads with AWS services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn Windows on AWS fundamentals - Understand Windows Server licensing on EC2 - Launch Windows Server EC2 instance 04/11/2025 04/11/2025 https://000093.awsstudygroup.com/vi/ 2 - Connect to Windows EC2 via RDP - Configure Windows Server roles and features - Deploy IIS web server on Windows EC2 05/11/2025 05/11/2025 AWS Windows Guide, FCJ Workshop 3 - Learn AWS Managed Microsoft AD basics - Understand Active Directory concepts - Create AWS Managed Microsoft AD directory 06/11/2025 06/11/2025 AWS Directory Service Docs, FCJ Workshop 4 - Join Windows EC2 to domain - Configure domain users and groups - Implement Group Policy Objects (GPOs) 07/11/2025 07/11/2025 AWS Managed AD Guide, FCJ Workshop 5 - Mini practice: + Deploy Windows application on EC2 + Integrate EC2 with Managed Microsoft AD + Setup domain authentication + Test SSO and centralized user management 08/11/2025 08/11/2025 AWS Enterprise Best Practices, FCJ Workshop Week 9 Achievements: Successfully deployed Windows Server on EC2 instances. Learned to connect and manage Windows instances via RDP. Mastered AWS Managed Microsoft AD for directory services. Integrated Windows EC2 instances with Active Directory domain. Implemented centralized authentication and Group Policy management. Summary: By the end of Week 9, I was able to:\nLaunch and configure Windows Server instances on AWS. Deploy Windows applications and IIS web servers on EC2. Create and manage AWS Managed Microsoft AD directories. Join Windows instances to Active Directory domains. Implement enterprise identity management using AWS Directory Services. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Understand hybrid Active Directory integration with AWS. Learn to integrate AWS Managed Microsoft AD with on-premises AD. Master AD Connector and Simple AD services. Practice advanced directory services scenarios and troubleshooting. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn hybrid AD integration concepts - Understand AD trust relationships - Configure AD trust between AWS and on-premises 11/11/2025 11/11/2025 AWS Hybrid AD Docs, FCJ Workshop 2 - Learn AD Connector service - Understand differences: Managed AD vs AD Connector vs Simple AD - Setup AD Connector for on-premises integration 12/11/2025 12/11/2025 AWS Directory Service Guide, FCJ Workshop 3 - Configure cross-domain authentication - Setup SSO for AWS applications - Integrate with AWS WorkSpaces and other services 13/11/2025 13/11/2025 AWS SSO Integration Docs, FCJ Workshop 4 - Practice directory services troubleshooting - Monitor directory health and performance - Implement backup and recovery strategies 14/11/2025 14/11/2025 AWS Directory Service Best Practices, FCJ Workshop 5 - Mini practice: + Setup hybrid AD environment + Configure AD trust relationships + Test cross-domain authentication + Review all directory services use cases 15/11/2025 15/11/2025 AWS Enterprise Architecture, FCJ Workshop Week 10 Achievements: Successfully understood hybrid Active Directory architectures. Learned to configure AD trust relationships between AWS and on-premises. Mastered AD Connector for hybrid directory integration. Implemented cross-domain authentication and SSO. Practiced troubleshooting and monitoring directory services. Summary: By the end of Week 10, I was able to:\nDesign and implement hybrid Active Directory solutions. Configure trust relationships between AWS Managed AD and on-premises AD. Choose appropriate directory service (Managed AD, AD Connector, Simple AD) based on requirements. Integrate directory services with AWS applications for SSO. Monitor, troubleshoot, and maintain directory services in production environments. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Understand high availability architecture principles on AWS. Learn to design and implement multi-tier web applications. Master Elastic Load Balancing (ALB, NLB, CLB) for traffic distribution. Implement disaster recovery and backup strategies. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Learn high availability concepts - Understand fault tolerance and resilience - Study AWS Well-Architected Framework (Reliability Pillar) 18/11/2025 18/11/2025 AWS Well-Architected Framework, FCJ Workshop 2 - Learn Elastic Load Balancing fundamentals - Understand ALB, NLB, and CLB differences - Create Application Load Balancer 19/11/2025 19/11/2025 AWS ELB Docs, FCJ Workshop 3 - Design multi-tier architecture - Setup web tier, app tier, and database tier - Configure load balancing across multiple AZs 20/11/2025 20/11/2025 AWS Multi-Tier Architecture Guide, FCJ Workshop 4 - Implement disaster recovery strategies - Configure automated backups (RDS, EBS snapshots) - Setup cross-region replication 21/11/2025 21/11/2025 AWS Disaster Recovery Docs, FCJ Workshop 5 - Mini practice: + Build highly available web application + Configure ALB with Auto Scaling + Setup Multi-AZ RDS with read replicas + Test failover scenarios 22/11/2025 22/11/2025 AWS HA Best Practices, FCJ Workshop Week 11 Achievements: Successfully understood high availability and fault tolerance principles. Learned to implement Elastic Load Balancing with multiple load balancer types. Mastered multi-tier architecture design across multiple Availability Zones. Implemented disaster recovery strategies with automated backups. Built production-ready highly available web applications. Summary: By the end of Week 11, I was able to:\nDesign highly available architectures using AWS best practices. Configure Application Load Balancers for intelligent traffic distribution. Build multi-tier applications with web, application, and database layers. Implement automated backup and disaster recovery strategies. Test and validate failover scenarios for production readiness. "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Build a comprehensive capstone project integrating all learned AWS services. Review and consolidate 12 weeks of AWS knowledge. Practice AWS best practices and architecture patterns. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Plan capstone project: - Design complete application architecture - Document infrastructure requirements - Create deployment checklist 25/11/2025 25/11/2025 AWS Architecture Center, FCJ Workshop 2 - Deploy capstone project - Day 1: - Setup VPC, subnets, and networking - Deploy EC2 with Auto Scaling and Load Balancer - Configure RDS Multi-AZ database 26/11/2025 26/11/2025 All Previous Workshops 3 - Deploy capstone project - Day 2: - Integrate ElastiCache for performance - Setup CloudFront and Route 53 - Configure monitoring with CloudWatch 27/11/2025 27/11/2025 AWS Best Practices, FCJ Workshop 4 - Review and optimization: - Review all AWS services learned - Optimize costs and performance - Document architecture and lessons learned 28/11/2025 28/11/2025 AWS Cost Optimization Guide 5 - Final review and certification prep: + Take AWS practice exams + Review AWS Certified Solutions Architect topics + Clean up all resources + Celebrate completion! 🎉 29/11/2025 29/11/2025 AWS Certification Resources, FCJ Workshop Week 12 Achievements: Successfully completed comprehensive capstone project using multiple AWS services. Reviewed and consolidated all 12 weeks of AWS knowledge. Implemented AWS best practices in production-ready architecture. Documented complete infrastructure and deployment process. Prepared for AWS Certification examination. Summary: By the end of Week 12, I was able to:\nDesign and deploy complete, production-ready AWS applications. Integrate multiple AWS services: VPC, EC2, RDS, S3, CloudFront, Route 53, Auto Scaling, CloudWatch. Implement high availability, scalability, and disaster recovery. Optimize costs and performance using AWS best practices. Demonstrate comprehensive understanding of AWS cloud architecture. 12-Week Program Completion: Total AWS Services Mastered: 15+ services Key Competencies Achieved:\nAWS Account Management \u0026amp; Security (IAM, MFA, Budgets) Networking (VPC, Route 53, CloudFront) Compute (EC2, Auto Scaling, Lightsail, Lambda@Edge) Storage (S3, EBS) Database (RDS, DynamoDB, ElastiCache) Monitoring \u0026amp; Management (CloudWatch, CLI) High Availability \u0026amp; Scalability Windows Workloads \u0026amp; Directory Services Content Delivery \u0026amp; Edge Computing Next Steps:\nContinue hands-on practice with real-world projects Join AWS Community and contribute Explore advanced AWS services (EKS, ECS, Lambda, Step Functions) "
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://Dung-Le-1234.github.io/FCAJ-Full-Workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]